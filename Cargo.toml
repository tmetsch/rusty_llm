[package]
name = "rusty_llm"
version = "0.6.0"
edition = "2021"

[dependencies]
actix-web = { version = "4.9.0" }
encoding_rs = { version = "0.8.35" }
env_logger = { version = "0.11.5" }
futures = { version = "0.3.31" }
lazy_static = { version = "1.5.0" }
# So OpenCL support for llama.cpp only offloads matrix multiplications - so it is not the fastest.
# SYCL support is a pain to get working as there are some undefined reference in the OneAPI libs.
# but hey, I learned a lot about build scripts along the way.
# Vulkan seems to work for Intel GPUs - just enable the feature: features = ["vulkan"].
# For some reason it slows down the code when not using GPUs btw.
llama-cpp-2 = { version = "0.1.86" }
log = { version = "0.4.22" }
prometheus = { version = "0.13.4" }
serde_json = { version = "1.0.133" }
serde = { version = "1.0.216", features = ["derive"] }
strfmt = { version = "0.2.4" }
tokio = { version = "1.42.0" }

[lib]
name = "rusty_llm"
path = "src/lib.rs"
doctest = false
